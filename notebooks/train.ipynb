{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Mixture of Experts (MoE) LLM in Google Colab\n",
    "\n",
    "This notebook demonstrates how to train a Mixture of Experts (MoE) Large Language Model in Google Colab with memory-efficient techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the required dependencies and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/your-username/moe-llm.git\n",
    "!cd moe-llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import torch\n",
    "import logging\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Import our modules\n",
    "from model.config import MoEConfig, TrainingConfig\n",
    "from model.model import MoELLM\n",
    "from data.dataset import load_and_prepare_datasets, create_dataloaders\n",
    "from data.tokenizer import get_tokenizer\n",
    "from training.trainer import MoETrainer, create_optimizer, create_lr_scheduler\n",
    "from training.memory_utils import get_memory_stats, print_memory_stats, optimize_memory_efficiency\n",
    "from utils.logging import configure_logging, TensorboardLogger, WandBLogger\n",
    "\n",
    "# Configure logging\n",
    "logger = configure_logging(log_level=\"INFO\", log_file=\"logs/training.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Available Resources\n",
    "\n",
    "Let's check the available resources in Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "# Check CPU resources\n",
    "import psutil\n",
    "print(f\"CPU count: {psutil.cpu_count()}\")\n",
    "print(f\"RAM: {psutil.virtual_memory().total / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Model and Training\n",
    "\n",
    "Let's configure the model and training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "model_config = MoEConfig(\n",
    "    vocab_size=32000,\n",
    "    hidden_size=1024,  # Reduced for Colab\n",
    "    intermediate_size=2816,  # Reduced for Colab\n",
    "    num_hidden_layers=16,  # Reduced for Colab\n",
    "    num_attention_heads=16,  # Reduced for Colab\n",
    "    num_experts=8,\n",
    "    num_experts_per_token=2,\n",
    "    max_position_embeddings=2048,  # Reduced for Colab\n",
    "    max_sequence_length=2048,  # Reduced for Colab\n",
    "    use_mla=True,  # Multi-head Latent Attention\n",
    "    mla_dim=64,  # Reduced for Colab\n",
    "    use_mtp=True,  # Multi-Token Prediction\n",
    "    mtp_num_tokens=2  # Reduced for Colab\n",
    ")\n",
    "\n",
    "# Training configuration\n",
    "training_config = TrainingConfig(\n",
    "    batch_size=2,  # Reduced for Colab\n",
    "    gradient_accumulation_steps=8,  # Increased for Colab\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    max_steps=10000,  # Reduced for Colab\n",
    "    warmup_steps=500,\n",
    "    optimizer_type=\"8bit-adam\",  # Memory-efficient optimizer\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    use_gradient_checkpointing=True,  # Memory optimization\n",
    "    mixed_precision=\"bf16\",  # Memory optimization\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    max_seq_length=2048,  # Reduced for Colab\n",
    "    preprocessing_num_workers=2  # Reduced for Colab\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer and Datasets\n",
    "\n",
    "Let's load the tokenizer and prepare the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = get_tokenizer(\n",
    "    tokenizer_name_or_path=\"EleutherAI/gpt-neox-20b\",  # Using an existing tokenizer\n",
    "    use_fast=True\n",
    ")\n",
    "\n",
    "# Update model config with tokenizer vocab size\n",
    "model_config.vocab_size = len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare datasets\n",
    "# Using a smaller dataset for Colab\n",
    "dataset_paths = [\"roneneldan/TinyStories\"]\n",
    "\n",
    "datasets = load_and_prepare_datasets(\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_paths=dataset_paths,\n",
    "    max_seq_length=training_config.max_seq_length,\n",
    "    streaming=False,  # Set to True for larger datasets\n",
    "    text_column=\"text\",\n",
    "    preprocessing_num_workers=training_config.preprocessing_num_workers\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "dataloaders = create_dataloaders(\n",
    "    datasets=datasets,\n",
    "    batch_size=training_config.batch_size,\n",
    "    num_workers=training_config.preprocessing_num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model\n",
    "\n",
    "Let's initialize the model with the configured parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = MoELLM(model_config)\n",
    "\n",
    "# Apply memory optimizations\n",
    "model = optimize_memory_efficiency(model)\n",
    "\n",
    "# Print model size\n",
    "from training.memory_utils import get_model_size\n",
    "model_size = get_model_size(model)\n",
    "print(f\"Model size: {model_size['total_parameters'] / 1e6:.2f}M parameters\")\n",
    "print(f\"Parameter size: {model_size['parameter_size_mb']:.2f} MB\")\n",
    "print(f\"Estimated total size: {model_size['estimated_total_size_mb']:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Training\n",
    "\n",
    "Let's set up the optimizer, learning rate scheduler, and trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizer\n",
    "optimizer = create_optimizer(\n",
    "    model=model,\n",
    "    learning_rate=training_config.learning_rate,\n",
    "    weight_decay=training_config.weight_decay,\n",
    "    optimizer_type=training_config.optimizer_type\n",
    ")\n",
    "\n",
    "# Calculate number of training steps\n",
    "if hasattr(dataloaders[\"train\"], \"__len__\"):\n",
    "    num_training_steps = len(dataloaders[\"train\"]) * training_config.max_steps\n",
    "else:\n",
    "    num_training_steps = training_config.max_steps\n",
    "\n",
    "# Create learning rate scheduler\n",
    "lr_scheduler = create_lr_scheduler(\n",
    "    optimizer=optimizer,\n",
    "    num_training_steps=num_training_steps,\n",
    "    warmup_steps=training_config.warmup_steps,\n",
    "    lr_scheduler_type=training_config.lr_scheduler_type\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = MoETrainer(\n",
    "    model=model,\n",
    "    train_dataloader=dataloaders[\"train\"],\n",
    "    eval_dataloader=dataloaders.get(\"validation\"),\n",
    "    optimizer=optimizer,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    config=training_config,\n",
    "    use_wandb=False  # Set to True to use Weights & Biases\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "Now, let's train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model\n",
    "\n",
    "Let's evaluate the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "eval_metrics = trainer.evaluate()\n",
    "print(f\"Evaluation metrics: {eval_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model\n",
    "\n",
    "Let's save the trained model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "from utils.checkpoint import save_model_for_inference\n",
    "\n",
    "save_model_for_inference(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    output_dir=\"model\",\n",
    "    save_format=\"pytorch\",\n",
    "    quantization=\"int8\"  # Apply quantization for smaller model size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text with the Model\n",
    "\n",
    "Let's generate some text with the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "prompt = \"Once upon a time, there was a\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "# Generate text\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=100,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "# Decode output\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've trained a Mixture of Experts (MoE) Large Language Model in Google Colab with memory-efficient techniques. The model can be further improved by training on larger datasets and with more resources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
