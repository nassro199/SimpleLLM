{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/nassro199/SimpleLLM/blob/main/notebooks/SimpleLLM_colab_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training SimpleLLM in Google Colab\n",
    "\n",
    "This notebook provides a complete end-to-end workflow for training the SimpleLLM (a Mixture of Experts Large Language Model) in Google Colab. The implementation is optimized for Colab's resource constraints and includes memory-efficient techniques to enable training of large-scale models.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. **Setup Environment**: Clone the repository and install dependencies\n",
    "2. **Check Available Resources**: Verify GPU and memory availability\n",
    "3. **Configure Model**: Set up model architecture and training parameters\n",
    "4. **Prepare Data**: Download and preprocess training data\n",
    "5. **Train Model**: Train the model with memory-efficient techniques\n",
    "6. **Generate Text**: Test the model with simple text generation\n",
    "7. **Save and Export**: Save the model for future use\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment\n",
    "\n",
    "First, let's check what GPU we have available and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's clone the repository and install the required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/nassro199/SimpleLLM.git\n",
    "%cd SimpleLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# Install additional dependencies for Colab\n",
    "!pip install accelerate bitsandbytes sentencepiece datasets wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check Available Resources\n",
    "\n",
    "Let's check the available resources in detail to help us configure our model appropriately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import psutil\n",
    "import os\n",
    "import GPUtil\n",
    "\n",
    "# Check CPU resources\n",
    "print(f\"CPU Count: {psutil.cpu_count()}\")\n",
    "print(f\"Available Memory: {psutil.virtual_memory().available / (1024**3):.2f} GB\")\n",
    "\n",
    "# Check GPU resources\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nGPU Information:\")\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB\")\n",
    "    \n",
    "    # Get more detailed GPU info\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    for i, gpu in enumerate(gpus):\n",
    "        print(f\"\\nGPU {i}: {gpu.name}\")\n",
    "        print(f\"Memory Free: {gpu.memoryFree} MB\")\n",
    "        print(f\"Memory Used: {gpu.memoryUsed} MB\")\n",
    "        print(f\"Memory Total: {gpu.memoryTotal} MB\")\n",
    "        print(f\"GPU Utilization: {gpu.load*100:.2f}%\")\n",
    "else:\n",
    "    print(\"No GPU available. Please change runtime type to include a GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the available resources, we'll configure our model. Let's set up some helper functions to monitor memory usage during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_memory_summary():\n",
    "    \"\"\"Print a summary of GPU memory usage.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"\\nGPU Memory Summary:\")\n",
    "        print(f\"Allocated: {torch.cuda.memory_allocated() / (1024**3):.2f} GB\")\n",
    "        print(f\"Cached: {torch.cuda.memory_reserved() / (1024**3):.2f} GB\")\n",
    "        print(f\"Max Allocated: {torch.cuda.max_memory_allocated() / (1024**3):.2f} GB\")\n",
    "        print(f\"Max Cached: {torch.cuda.max_memory_reserved() / (1024**3):.2f} GB\")\n",
    "    else:\n",
    "        print(\"No GPU available.\")\n",
    "\n",
    "# Import our memory utilities\n",
    "from training.memory_utils import get_memory_stats, print_memory_stats, clear_memory\n",
    "\n",
    "# Clear memory and print initial stats\n",
    "clear_memory()\n",
    "print_gpu_memory_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Model\n",
    "\n",
    "Now, let's configure our model architecture and training parameters based on the available resources. We'll use a smaller configuration for Colab's constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our configuration classes\n",
    "from model.config import MoEConfig, TrainingConfig\n",
    "\n",
    "# Define model configuration\n",
    "# Adjust these parameters based on your available GPU memory\n",
    "model_config = MoEConfig(\n",
    "    vocab_size=32000,\n",
    "    hidden_size=768,  # Reduced for Colab\n",
    "    intermediate_size=2048,  # Reduced for Colab\n",
    "    num_hidden_layers=12,  # Reduced for Colab\n",
    "    num_attention_heads=12,  # Reduced for Colab\n",
    "    num_experts=8,\n",
    "    num_experts_per_token=2,\n",
    "    expert_capacity=0,  # Auto-calculate\n",
    "    router_jitter_noise=0.1,\n",
    "    router_z_loss_coef=0.001,\n",
    "    router_aux_loss_coef=0.001,\n",
    "    max_position_embeddings=2048,  # Reduced for Colab\n",
    "    max_sequence_length=2048,  # Reduced for Colab\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_dropout_prob=0.1,\n",
    "    use_rms_norm=True,\n",
    "    position_embedding_type=\"rotary\",\n",
    "    rotary_dim=64,  # Reduced for Colab\n",
    "    use_mla=True,  # Multi-head Latent Attention\n",
    "    mla_dim=64,  # Reduced for Colab\n",
    "    use_mtp=True,  # Multi-Token Prediction\n",
    "    mtp_num_tokens=2  # Reduced for Colab\n",
    ")\n",
    "\n",
    "# Define training configuration\n",
    "training_config = TrainingConfig(\n",
    "    batch_size=2,  # Small batch size for Colab\n",
    "    gradient_accumulation_steps=8,  # Accumulate gradients to simulate larger batch\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    max_steps=5000,  # Reduced for Colab\n",
    "    warmup_steps=200,\n",
    "    optimizer_type=\"8bit-adam\",  # Memory-efficient optimizer\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    use_gradient_checkpointing=True,  # Memory optimization\n",
    "    mixed_precision=\"bf16\",  # Memory optimization (use \"fp16\" if bf16 not supported)\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    max_seq_length=2048,  # Reduced for Colab\n",
    "    preprocessing_num_workers=2  # Reduced for Colab\n",
    ")\n",
    "\n",
    "# Print configurations\n",
    "print(\"Model Configuration:\")\n",
    "for key, value in vars(model_config).items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nTraining Configuration:\")\n",
    "for key, value in vars(training_config).items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Data\n",
    "\n",
    "Now, let's prepare the data for training. We'll use a smaller dataset for demonstration purposes, but you can replace it with your own dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data utilities\n",
    "from data.tokenizer import get_tokenizer\n",
    "from data.dataset import load_and_prepare_datasets, create_dataloaders\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load tokenizer\n",
    "# We'll use an existing tokenizer for simplicity\n",
    "tokenizer = get_tokenizer(\n",
    "    tokenizer_name_or_path=\"EleutherAI/gpt-neo-1.3B\",  # Using an existing tokenizer\n",
    "    use_fast=True\n",
    ")\n",
    "\n",
    "# Update model config with tokenizer vocab size\n",
    "model_config.vocab_size = len(tokenizer)\n",
    "print(f\"Updated vocab size to {model_config.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a small dataset for training. For this example, we'll use the TinyStories dataset, which is small enough for Colab but still useful for training language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small dataset for demonstration\n",
    "# You can replace this with your own dataset\n",
    "print(\"Loading dataset...\")\n",
    "dataset_paths = [\"roneneldan/TinyStories\"]\n",
    "\n",
    "datasets = load_and_prepare_datasets(\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_paths=dataset_paths,\n",
    "    max_seq_length=training_config.max_seq_length,\n",
    "    streaming=False,  # Set to True for larger datasets\n",
    "    text_column=\"text\",\n",
    "    preprocessing_num_workers=training_config.preprocessing_num_workers\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "dataloaders = create_dataloaders(\n",
    "    datasets=datasets,\n",
    "    batch_size=training_config.batch_size,\n",
    "    num_workers=training_config.preprocessing_num_workers\n",
    ")\n",
    "\n",
    "# Print dataset information\n",
    "print(\"\\nDataset Information:\")\n",
    "for split, dataset in datasets.items():\n",
    "    print(f\"  {split}: {len(dataset)} examples\")\n",
    "\n",
    "# Show a sample from the dataset\n",
    "print(\"\\nSample from dataset:\")\n",
    "sample = datasets[\"train\"][0]\n",
    "print(f\"Input IDs shape: {sample['input_ids'].shape}\")\n",
    "print(f\"Attention Mask shape: {sample['attention_mask'].shape}\")\n",
    "print(f\"Labels shape: {sample['labels'].shape}\")\n",
    "\n",
    "# Decode a sample to show the text\n",
    "decoded_text = tokenizer.decode(sample['input_ids'][:50])  # Show first 50 tokens\n",
    "print(f\"\\nDecoded sample (first 50 tokens): {decoded_text}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Model\n",
    "\n",
    "Now, let's initialize our SimpleLLM model with the configured parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model\n",
    "from model.model import MoELLM\n",
    "from training.memory_utils import get_model_size, optimize_memory_efficiency\n",
    "\n",
    "# Initialize model\n",
    "print(\"Initializing model...\")\n",
    "model = MoELLM(model_config)\n",
    "\n",
    "# Apply memory optimizations\n",
    "model = optimize_memory_efficiency(model)\n",
    "\n",
    "# Get model size information\n",
    "model_size_info = get_model_size(model)\n",
    "print(\"\\nModel Size Information:\")\n",
    "print(f\"  Total Parameters: {model_size_info['total_parameters'] / 1e6:.2f}M\")\n",
    "print(f\"  Trainable Parameters: {model_size_info['trainable_parameters'] / 1e6:.2f}M\")\n",
    "print(f\"  Parameter Size: {model_size_info['parameter_size_mb']:.2f} MB\")\n",
    "print(f\"  Estimated Total Size: {model_size_info['estimated_total_size_mb']:.2f} MB\")\n",
    "\n",
    "# Check memory usage after model initialization\n",
    "print_gpu_memory_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Set Up Training\n",
    "\n",
    "Now, let's set up the optimizer, learning rate scheduler, and trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import training utilities\n",
    "from training.trainer import MoETrainer, create_optimizer, create_lr_scheduler\n",
    "from utils.logging import configure_logging\n",
    "\n",
    "# Configure logging\n",
    "logger = configure_logging(log_level=\"INFO\", log_file=\"logs/training.log\")\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = create_optimizer(\n",
    "    model=model,\n",
    "    learning_rate=training_config.learning_rate,\n",
    "    weight_decay=training_config.weight_decay,\n",
    "    optimizer_type=training_config.optimizer_type\n",
    ")\n",
    "\n",
    "# Calculate number of training steps\n",
    "if hasattr(dataloaders[\"train\"], \"__len__\"):\n",
    "    num_training_steps = len(dataloaders[\"train\"]) * training_config.max_steps\n",
    "else:\n",
    "    num_training_steps = training_config.max_steps\n",
    "\n",
    "# Create learning rate scheduler\n",
    "lr_scheduler = create_lr_scheduler(\n",
    "    optimizer=optimizer,\n",
    "    num_training_steps=num_training_steps,\n",
    "    warmup_steps=training_config.warmup_steps,\n",
    "    lr_scheduler_type=training_config.lr_scheduler_type\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = MoETrainer(\n",
    "    model=model,\n",
    "    train_dataloader=dataloaders[\"train\"],\n",
    "    eval_dataloader=dataloaders.get(\"validation\"),\n",
    "    optimizer=optimizer,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    config=training_config,\n",
    "    use_wandb=False  # Set to True if you want to use Weights & Biases\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train the Model\n",
    "\n",
    "Now, let's train the model. This will take some time, so be patient. You can adjust the number of steps based on your available time and resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Connect to Weights & Biases for experiment tracking\n",
    "# Uncomment and run this cell if you want to use W&B\n",
    "\"\"\"\n",
    "import wandb\n",
    "wandb.login()\n",
    "wandb.init(\n",
    "    project=\"simplellm-colab\",\n",
    "    name=\"simplellm-training\",\n",
    "    config={\n",
    "        \"model_config\": vars(model_config),\n",
    "        \"training_config\": vars(training_config)\n",
    "    }\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate Text with the Model\n",
    "\n",
    "Let's generate some text with our trained model to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define prompts for text generation\n",
    "prompts = [\n",
    "    \"Once upon a time, there was a\",\n",
    "    \"The best way to learn is to\",\n",
    "    \"In the future, artificial intelligence will\"\n",
    "]\n",
    "\n",
    "# Generate text for each prompt\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    \n",
    "    # Tokenize prompt\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    \n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=100,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.2\n",
    "        )\n",
    "    \n",
    "    # Decode output\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    print(f\"Generated: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save the Model\n",
    "\n",
    "Finally, let's save the trained model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import checkpoint utilities\n",
    "from utils.checkpoint import save_model_for_inference\n",
    "\n",
    "# Save the model\n",
    "print(\"Saving model...\")\n",
    "output_dir = \"trained_model\"\n",
    "save_model_for_inference(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    output_dir=output_dir,\n",
    "    save_format=\"pytorch\",\n",
    "    quantization=\"int8\"  # Apply quantization for smaller model size\n",
    ")\n",
    "print(f\"Model saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Download the Model\n",
    "\n",
    "You can download the trained model to your local machine for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the model directory for easier download\n",
    "!zip -r trained_model.zip trained_model\n",
    "\n",
    "# Download the model\n",
    "from google.colab import files\n",
    "files.download('trained_model.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusion\n",
    "\n",
    "Congratulations! You've successfully trained the SimpleLLM model in Google Colab. Here's a summary of what we've accomplished:\n",
    "\n",
    "1. Set up the environment and installed dependencies\n",
    "2. Configured a memory-efficient MoE model architecture\n",
    "3. Prepared a dataset for training\n",
    "4. Trained the model with memory-efficient techniques\n",
    "5. Generated text with the trained model\n",
    "6. Saved the model for future use\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try training on larger datasets for better performance\n",
    "- Experiment with different model configurations\n",
    "- Fine-tune the model on specific tasks\n",
    "- Use the model for various text generation tasks\n",
    "- Deploy the model for inference\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [GitHub Repository](https://github.com/nassro199/SimpleLLM)\n",
    "- [Technical Report](https://github.com/nassro199/SimpleLLM/blob/main/report/technical_report.md)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
