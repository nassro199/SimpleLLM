{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating a Mixture of Experts (MoE) LLM\n",
    "\n",
    "This notebook demonstrates how to evaluate a Mixture of Experts (MoE) Large Language Model on various benchmarks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the required dependencies and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/your-username/moe-llm.git\n",
    "!cd moe-llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import torch\n",
    "import logging\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Import our modules\n",
    "from model.config import MoEConfig, EvaluationConfig\n",
    "from model.model import MoELLM\n",
    "from data.tokenizer import get_tokenizer\n",
    "from evaluation.benchmarks import BenchmarkEvaluator\n",
    "from evaluation.perplexity import calculate_perplexity\n",
    "from utils.logging import configure_logging\n",
    "from utils.visualization import plot_benchmark_results, plot_comparison_with_deepseek\n",
    "\n",
    "# Configure logging\n",
    "logger = configure_logging(log_level=\"INFO\", log_file=\"logs/evaluation.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer\n",
    "\n",
    "Let's load the trained model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = get_tokenizer(\n",
    "    tokenizer_name_or_path=\"model\",  # Path to the saved model\n",
    "    use_fast=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model configuration\n",
    "model_config = MoEConfig.from_pretrained(\"model\")\n",
    "\n",
    "# Initialize model\n",
    "model = MoELLM(model_config)\n",
    "\n",
    "# Load model weights\n",
    "model.load_state_dict(torch.load(\"model/pytorch_model.bin\", map_location=\"cpu\"))\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Perplexity\n",
    "\n",
    "Let's calculate the perplexity of the model on a validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "validation_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"validation\")\n",
    "\n",
    "# Calculate perplexity\n",
    "perplexity_metrics = calculate_perplexity(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    texts=validation_dataset[\"text\"],\n",
    "    device=device,\n",
    "    batch_size=1,\n",
    "    stride=512,\n",
    "    max_length=1024\n",
    ")\n",
    "\n",
    "print(f\"Perplexity: {perplexity_metrics['perplexity']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Benchmarks\n",
    "\n",
    "Let's evaluate the model on various benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize benchmark evaluator\n",
    "evaluator = BenchmarkEvaluator(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    output_dir=\"benchmark_results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on MMLU\n",
    "mmlu_metrics = evaluator.evaluate_mmlu(\n",
    "    data_path=\"cais/mmlu\",\n",
    "    num_few_shot=5,\n",
    "    batch_size=4\n",
    ")\n",
    "\n",
    "print(f\"MMLU accuracy: {mmlu_metrics['mmlu_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on GSM8K\n",
    "gsm8k_metrics = evaluator.evaluate_gsm8k(\n",
    "    data_path=\"gsm8k\",\n",
    "    split=\"test\",\n",
    "    num_few_shot=8,\n",
    "    batch_size=4\n",
    ")\n",
    "\n",
    "print(f\"GSM8K accuracy: {gsm8k_metrics['gsm8k_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on MATH\n",
    "math_metrics = evaluator.evaluate_math(\n",
    "    data_path=\"hendrycks_math\",\n",
    "    split=\"test\",\n",
    "    num_few_shot=4,\n",
    "    batch_size=4\n",
    ")\n",
    "\n",
    "print(f\"MATH accuracy: {math_metrics['math_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on BBH\n",
    "bbh_metrics = evaluator.evaluate_bbh(\n",
    "    data_path=\"lukaemon/bbh\",\n",
    "    num_few_shot=3,\n",
    "    batch_size=4\n",
    ")\n",
    "\n",
    "print(f\"BBH accuracy: {bbh_metrics['bbh_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine and Visualize Results\n",
    "\n",
    "Let's combine all the benchmark results and visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all benchmark results\n",
    "all_metrics = {}\n",
    "all_metrics.update(mmlu_metrics)\n",
    "all_metrics.update(gsm8k_metrics)\n",
    "all_metrics.update(math_metrics)\n",
    "all_metrics.update(bbh_metrics)\n",
    "all_metrics[\"perplexity\"] = perplexity_metrics[\"perplexity\"]\n",
    "\n",
    "# Save all results\n",
    "with open(\"benchmark_results/all_benchmark_results.json\", \"w\") as f:\n",
    "    json.dump(all_metrics, f, indent=2)\n",
    "\n",
    "# Plot benchmark results\n",
    "plot_path = plot_benchmark_results(\n",
    "    results_file=\"benchmark_results/all_benchmark_results.json\",\n",
    "    output_dir=\"plots\",\n",
    "    output_name=\"benchmark_results.png\"\n",
    ")\n",
    "\n",
    "# Display the plot\n",
    "from IPython.display import Image\n",
    "Image(filename=plot_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with DeepSeek-V3\n",
    "\n",
    "Let's compare our results with DeepSeek-V3's published results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepSeek-V3 published results\n",
    "# These are placeholder values - replace with actual published results\n",
    "deepseek_results = {\n",
    "    \"mmlu_accuracy\": 0.80,\n",
    "    \"gsm8k_accuracy\": 0.85,\n",
    "    \"math_accuracy\": 0.50,\n",
    "    \"bbh_accuracy\": 0.75\n",
    "}\n",
    "\n",
    "# Plot comparison\n",
    "comparison_plot_path = plot_comparison_with_deepseek(\n",
    "    our_results=all_metrics,\n",
    "    deepseek_results=deepseek_results,\n",
    "    output_dir=\"plots\",\n",
    "    output_name=\"comparison_with_deepseek.png\"\n",
    ")\n",
    "\n",
    "# Display the plot\n",
    "Image(filename=comparison_plot_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Performance Discrepancies\n",
    "\n",
    "Let's analyze the performance discrepancies between our model and DeepSeek-V3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance discrepancies\n",
    "discrepancies = {}\n",
    "for key in deepseek_results:\n",
    "    if key in all_metrics:\n",
    "        discrepancies[key] = all_metrics[key] - deepseek_results[key]\n",
    "\n",
    "# Print discrepancies\n",
    "print(\"Performance Discrepancies:\")\n",
    "for key, value in discrepancies.items():\n",
    "    print(f\"{key}: {value:.4f} ({'+' if value >= 0 else ''}{value * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text with the Model\n",
    "\n",
    "Let's generate some text with the model to demonstrate its capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompts\n",
    "prompts = [\n",
    "    \"Explain the concept of a Mixture of Experts (MoE) architecture in simple terms.\",\n",
    "    \"What are the advantages of using a Mixture of Experts model compared to a dense model?\",\n",
    "    \"Solve the following math problem step by step: If a train travels at 60 mph for 3 hours and then at 80 mph for 2 hours, what is the average speed for the entire journey?\"\n",
    "]\n",
    "\n",
    "# Generate text for each prompt\n",
    "for prompt in prompts:\n",
    "    print(f\"Prompt: {prompt}\\n\")\n",
    "    \n",
    "    # Tokenize prompt\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    \n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=input_ids.shape[1] + 200,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9\n",
    "        )\n",
    "    \n",
    "    # Decode output\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Print generated text\n",
    "    print(f\"Generated text: {generated_text}\\n\")\n",
    "    print(\"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've evaluated a Mixture of Experts (MoE) Large Language Model on various benchmarks and compared its performance with DeepSeek-V3. We've also analyzed the performance discrepancies and demonstrated the model's text generation capabilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
